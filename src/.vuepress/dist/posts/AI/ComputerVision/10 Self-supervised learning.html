<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.15" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.52" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://mister-hope.github.io/blog/posts/AI/ComputerVision/10%20Self-supervised%20learning.html"><meta property="og:site_name" content="Krigo's ÂçöÂÆ¢"><meta property="og:description" content="Background: Supervised Learning relies on labeled data: costly and time-consuming to annotate. Unsupervised Learning seeks patterns in unlabeled data but doesn't predict specifi..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Krigo"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"","image":[""],"dateModified":null,"author":[{"@type":"Person","name":"Krigo","url":"https://mister-hope.com"}]}</script><title>Krigo's ÂçöÂÆ¢</title><meta name="description" content="Background: Supervised Learning relies on labeled data: costly and time-consuming to annotate. Unsupervised Learning seeks patterns in unlabeled data but doesn't predict specifi...">
    <link rel="preload" href="/blog/assets/style-DcxJrCA1.css" as="style"><link rel="stylesheet" href="/blog/assets/style-DcxJrCA1.css">
    <link rel="modulepreload" href="/blog/assets/app-B3e5LvWJ.js"><link rel="modulepreload" href="/blog/assets/10 Self-supervised learning.html-D1T7JUkk.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/blog/assets/index.html-B2TJ3WI5.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-D3owYInh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-D1ZloxrG.js" as="script"><link rel="prefetch" href="/blog/assets/BF.html-BgIhV3K1.js" as="script"><link rel="prefetch" href="/blog/assets/BacktestEva.html-B7z9IrSq.js" as="script"><link rel="prefetch" href="/blog/assets/Direction.html-BZkVvMb3.js" as="script"><link rel="prefetch" href="/blog/assets/Finance.html-BSPujuit.js" as="script"><link rel="prefetch" href="/blog/assets/Maths.html-DfHnD4hV.js" as="script"><link rel="prefetch" href="/blog/assets/Statistics.html-7NcF3KG_.js" as="script"><link rel="prefetch" href="/blog/assets/qf formulas.html-3Euf0Wbr.js" as="script"><link rel="prefetch" href="/blog/assets/HPC.html-NkZV7U-2.js" as="script"><link rel="prefetch" href="/blog/assets/Learning_Makefile.html-B0ptyLbT.js" as="script"><link rel="prefetch" href="/blog/assets/cmd.html-C79zr-dg.js" as="script"><link rel="prefetch" href="/blog/assets/conda.html-ly7C6Y2-.js" as="script"><link rel="prefetch" href="/blog/assets/git.html-1BJ16TbZ.js" as="script"><link rel="prefetch" href="/blog/assets/linux.html-AnJ1LL7H.js" as="script"><link rel="prefetch" href="/blog/assets/styles.html-B7Tm7wJf.js" as="script"><link rel="prefetch" href="/blog/assets/Blog_conf.html-Bto3kyed.js" as="script"><link rel="prefetch" href="/blog/assets/Algorithms.html-BRqaf5hV.js" as="script"><link rel="prefetch" href="/blog/assets/1 Introduction.html-CMkE3h_Z.js" as="script"><link rel="prefetch" href="/blog/assets/2 Image classification with linear classifiers.html-CMNTYaHF.js" as="script"><link rel="prefetch" href="/blog/assets/3 CNN.html-DvZr4has.js" as="script"><link rel="prefetch" href="/blog/assets/8 CNN Representation.html-Vbc4QgFD.js" as="script"><link rel="prefetch" href="/blog/assets/9 Transformer.html-BU9YQNRP.js" as="script"><link rel="prefetch" href="/blog/assets/Formula.html-BtMHLzYn.js" as="script"><link rel="prefetch" href="/blog/assets/SLDG - Introduction.html-BOPEXHN7.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-xzA9YzAH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-N1IuGP9v.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BcnpVFkj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CB4hCESQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BWRy1gVK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BL_cN8DX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CM4Ddkay.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BjWSsMDz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-D4Lwv0NY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B50bjFzm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BCmVd8-3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B-Nj5f6G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CNv-eM9Y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CcAYwbMy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CH7cHr3n.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DXkc7Tm8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ClN1KQcI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DKbEesSS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BzQBUYDD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BxvC-n4r.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ClWlAFEa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C5TknWWr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ERGVEr1P.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-GXRgw7eJ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Ë∑≥Ëá≥‰∏ªË¶ÅÂÖßÂÆπ</a><!--]--><div class="theme-container external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/"><img class="vp-nav-logo" src="https://theme-hope-assets.vuejs.press/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Krigo&#39;s ÂçöÂÆ¢</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/" aria-label="ÂçöÂÆ¢‰∏ªÈ°µ"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->ÂçöÂÆ¢‰∏ªÈ°µ<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="SLDG"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>SLDG<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/posts/MATH/SLDG/SLDG-Introduction.html" aria-label="/posts/MATH/SLDG/SLDG-Introduction.html"><!---->/posts/MATH/SLDG/SLDG-Introduction.html<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://theme-hope.vuejs.press/zh/" aria-label="V2 ÊñáÊ°£" rel="noopener noreferrer" target="_blank"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><!--]-->V2 ÊñáÊ°£<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/" aria-label="ÂçöÂÆ¢‰∏ªÈ°µ"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->ÂçöÂÆ¢‰∏ªÈ°µ<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/intro.html" aria-label="‰ªãÁªçÈ°µ"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-circle-info" style=""></span><!--]-->‰ªãÁªçÈ°µ<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">HPC</span><!----></p><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/posts/HPC/OpenMP/intro.html" aria-label="/posts/HPC/OpenMP/intro.html"><!---->/posts/HPC/OpenMP/intro.html<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/posts/HPC/OpenMP/OpenMP_upwind.html" aria-label="/posts/HPC/OpenMP/OpenMP_upwind.html"><!---->/posts/HPC/OpenMP/OpenMP_upwind.html<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">Computational Mathematics</span><!----></p><ul class="vp-sidebar-links"></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">SLDG</span><!----></p><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/posts/MATH/SLDG/1d_SLDG.html" aria-label="/posts/MATH/SLDG/1d_SLDG.html"><!---->/posts/MATH/SLDG/1d_SLDG.html<!----></a></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!----></h1><div class="page-info"><span class="page-author-info" aria-label="‰ΩúËÄÖüñä" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://mister-hope.com" target="_blank" rel="noopener noreferrer">Krigo</a></span><span property="author" content="Krigo"></span></span><!----><!----><!----><span class="page-reading-time-info" aria-label="ÈòÖËØªÊó∂Èó¥‚åõ" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>Â§ßÁ∫¶ 3 ÂàÜÈíü</span><meta property="timeRequired" content="PT3M"></span><!----><!----></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">Ê≠§È°µÂÜÖÂÆπ<button type="button" class="print-button" title="ÊâìÂç∞"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-core-concepts">1. Core Concepts</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-theoretical-insights-and-challenges">2. Theoretical Insights and Challenges</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-potential-exam-questions">3. Potential Exam Questions</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#study-tips">Study Tips</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#references">References</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><p><strong>Background</strong>:</p><ul><li><strong>Supervised Learning</strong> relies on labeled data: costly and time-consuming to annotate.</li><li><strong>Unsupervised Learning</strong> seeks patterns in unlabeled data but doesn&#39;t predict specific outcomes.</li><li><strong>Semi-Supervised Learning</strong>: Train jointly with some labeled data and (a lot) of unlabeled data.</li></ul><hr><h3 id="_1-core-concepts" tabindex="-1"><a class="header-anchor" href="#_1-core-concepts"><span>1. Core Concepts</span></a></h3><p><strong>Self-Supervised Learning (SSL)</strong>:</p><ul><li>Definition: Learning from raw data without annotations by using pretext tasks.</li><li>Advantages: Reduces dependency on labeled data, improves scalability.</li><li>Key Steps: Pretraining (on pretext tasks) ‚Üí Transfer to downstream tasks.</li><li>Types of Pretext Tasks: <img src="/blog/assets/lec10core-B6ZNDLrp.png" alt="img.png" loading="lazy"></li></ul><h4 id="_1-1-generative-tasks" tabindex="-1"><a class="header-anchor" href="#_1-1-generative-tasks"><span>1.1 Generative Tasks</span></a></h4><ol><li><p>Sparse AutoEncoders: Train an autoencoder to reconstruct inputs with sparse activations (mostly 0).</p></li><li><p>Denoising Autoencoder: Train an autoencoder to reconstruct noisy inputs (pixels randomly set to zero)</p></li><li><p>Masked Autoencoder: Masks portions of the input (e.g., image patches) and learns to reconstruct the missing parts.</p><ul><li>Efficient and scalable for large models like Vision Transformers (ViTs). <img src="/blog/assets/MAE-bzR6JbVc.png" alt="img.png" loading="lazy"></li></ul></li><li><p>Inpainting: Predict missing regions in an image. encoder + decoder</p><ol><li>Mask a portion of the input image (e.g., rectangular or irregular regions).</li><li>Train the model to predict the missing pixels based on the visible parts of the image.</li><li>Use a loss function (e.g., L2 loss (+ adversarial loss <em>best</em>)) to evaluate the quality of the reconstruction.</li></ol><ul><li>strength: rich feature learining, global+local context</li><li>lim: specific scenarios, computational cost, dependence on masking strat, generative overhead (precise pixel pred, may not align with downstream tasks)</li></ul></li><li><p>Colorization:</p><ol><li>For grayscale images, identify objects, then Predict colors. <img src="/blog/assets/color-DId-PwkW.png" alt="img.png" loading="lazy"></li><li>Split-Brain Autoencoder <img src="/blog/assets/color-splitb-CUfN6IyF.png" alt="img.png" loading="lazy"></li></ol></li></ol><h4 id="_1-2-discriminative-tasks" tabindex="-1"><a class="header-anchor" href="#_1-2-discriminative-tasks"><span>1.2 Discriminative Tasks</span></a></h4><ol><li>Contrastive Learning: Distinguish between similar (positive) and dissimilar (negative) samples. <ul><li>Contrastive loss: Each image predicts which caption matches</li><li>similarity: Euclidean distance</li><li>generate positive pairs: <ul><li>data augmentation</li><li><img src="/blog/assets/cons_data_aug-CXLERR7J.png" alt="img.png" tabindex="0" loading="lazy"><figcaption>img.png</figcaption></li></ul></li></ul></li><li>Context Prediction: Predict spatial &amp; semantic relationships between image patches. tasks, like predicting the <strong>relative</strong> location of one patch to another. Eg: Given two image patches, predict whether one patch is &quot;above,&quot; &quot;below,&quot; &quot;to the left,&quot; or &quot;to the right&quot; of the other. <strong>Subset: Jigsaw (ÊãºÂõæ)</strong>: shuffling multiple patches, and pred <strong>permutation</strong> of the patches. <ul><li>lim: only work on patches, not whole images! Because patches are localized regions, struggles to model holistic features like obj presence, size or overall composition.</li></ul></li><li>Rotation Prediction:RotNet, Classify the degree of rotation (e.g., 0¬∞, 90¬∞, 180¬∞, 270¬∞) applied to an image.</li><li>Deep Clustering: Group features and assign pseudo-labels to clusters. <ol><li>steps 1 Randomly initialize a CNN 2 Run many images through CNN, get their final-layer features 3 Cluster the features with K-Means; record cluster for each feature 4 Use cluster assignments as <em>pseudo labels</em> for each image; train the CNN to predict cluster assignments 5 repeat, GOTO 2</li></ol></li><li>Exemplar CNN: recognize the same data instance (e.g., image patches) despite different augmentations like cropping, rotation, scaling, or color jittering. <ul><li>obj:The model predicts which of the original image patches each augmented sample came from (an N-way classification task, where ùëÅ is the number of original patches).</li><li>strengths: instance-level features, augmentation-invariance, task-agnostic</li><li>lim: scalability <ul><li>(The final classification layer grows with the number of instances, making it harder to scale to massive datasets.)</li><li>Risk of Over-fitting</li></ul></li></ul></li></ol><h4 id="_1-3-multimodal-pretext-tasks" tabindex="-1"><a class="header-anchor" href="#_1-3-multimodal-pretext-tasks"><span>1.3 Multimodal Pretext Tasks</span></a></h4><ol><li>CLIP (Contrastive Language-Image Pretraining): Match images with their corresponding text descriptions. <ul><li>why lang.: <ul><li>semantic density, universality(des any concept), scalability(Non-experts can easily caption images; data can also be collected from the web at scale).</li><li>Language enables <strong>zero-shot classification</strong>: the ability of a model to classify data into categories it has never explicitly been trained on. Instead, the model relies on its pretraining and generalization abilities to infer relationships between new data and previously learned representations. <img src="/blog/assets/CLIP-Ve5-zagi.png" alt="img.png" loading="lazy">Dual encoders for image and text.</li><li>Strengths: <ul><li>Generalization across tasks. <ul><li>muti-modal, link vision and lang.</li></ul></li></ul></li><li>drawback: <ul><li>Dataset bias and closed nature of training data.</li><li>Dependency on large-scale data and compute.</li></ul></li></ul></li></ul></li><li>Video Context: Predict relationships between frames and their audio tracks.</li><li>Sound Correspondence: Learn relationships between an image and its associated ambient sound.</li></ol><hr><h3 id="_2-theoretical-insights-and-challenges" tabindex="-1"><a class="header-anchor" href="#_2-theoretical-insights-and-challenges"><span>2. Theoretical Insights and Challenges</span></a></h3><ol><li>Fair evaluation of SSL methods is very hard! No theory, so we need to rely on experiment</li><li>Variability in experimental setups: architecture, datasets, hyperparameters.</li><li>Current reliance on curated (data that has been carefully selected, organized, and preprocessed to meet specific criteria or objectives. This contrasts with raw or unprocessed data, which may include irrelevant, noisy, or unbalanced information.) datasets (like ImageNet).</li><li>Challenges scaling beyond curated or isolated datasets.</li></ol><hr><h3 id="_3-potential-exam-questions" tabindex="-1"><a class="header-anchor" href="#_3-potential-exam-questions"><span>3. Potential Exam Questions</span></a></h3><ol><li>Explain the key differences between supervised, unsupervised, and self-supervised learning.</li><li>Describe how contrastive learning works and its importance in SSL.</li><li>Explain how CLIP performs zero-shot classification.</li><li>Compare masked autoencoders with traditional autoencoders in terms of architecture and use cases.</li><li>Discuss the advantages and challenges of using SSL in multimodal learning.</li><li>Evaluate the trade-offs between generative and discriminative pretext tasks in SSL.</li></ol><hr><h3 id="study-tips" tabindex="-1"><a class="header-anchor" href="#study-tips"><span>Study Tips</span></a></h3><ul><li>Focus on understanding the logic behind pretext tasks and why they enable feature learning.</li><li>Memorize key architectures and loss functions (e.g., contrastive loss, InfoNCE).</li><li>Practice explaining applications like CLIP and masked autoencoders in simple terms.</li><li>Be ready to critique SSL methods, especially around scalability and bias.</li></ul><h3 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h3><p>https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/</p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope/edit/main/src/posts/AI/ComputerVision/10 Self-supervised learning.md" aria-label="Âú® GitHub ‰∏äÁºñËæëÊ≠§È°µ" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Âú® GitHub ‰∏äÁºñËæëÊ≠§È°µ<!----></a></div><div class="vp-meta-item git-info"><!----><!----></div></footer><!----><!----><!----><!--]--></main><!--]--><!----></div><!--]--><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-B3e5LvWJ.js" defer></script>
  </body>
</html>
