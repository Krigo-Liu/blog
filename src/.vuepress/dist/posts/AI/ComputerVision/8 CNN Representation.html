<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.15" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.52" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://mister-hope.github.io/blog/posts/AI/ComputerVision/8%20CNN%20Representation.html"><meta property="og:site_name" content="Krigo's 博客"><meta property="og:title" content="Understanding CNN Representations"><meta property="og:description" content="Understanding CNN Representations Usually, the first layer across different architectures of CNN seeks to learn edges and colors. However, deep layers learns more complex featur..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Krigo"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Understanding CNN Representations","image":[""],"dateModified":null,"author":[{"@type":"Person","name":"Krigo","url":"https://mister-hope.com"}]}</script><title>Understanding CNN Representations | Krigo's 博客</title><meta name="description" content="Understanding CNN Representations Usually, the first layer across different architectures of CNN seeks to learn edges and colors. However, deep layers learns more complex featur...">
    <link rel="preload" href="/blog/assets/style-DcxJrCA1.css" as="style"><link rel="stylesheet" href="/blog/assets/style-DcxJrCA1.css">
    <link rel="modulepreload" href="/blog/assets/app-B3e5LvWJ.js"><link rel="modulepreload" href="/blog/assets/8 CNN Representation.html-Vbc4QgFD.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/blog/assets/index.html-B2TJ3WI5.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-D3owYInh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-D1ZloxrG.js" as="script"><link rel="prefetch" href="/blog/assets/BF.html-BgIhV3K1.js" as="script"><link rel="prefetch" href="/blog/assets/BacktestEva.html-B7z9IrSq.js" as="script"><link rel="prefetch" href="/blog/assets/Direction.html-BZkVvMb3.js" as="script"><link rel="prefetch" href="/blog/assets/Finance.html-BSPujuit.js" as="script"><link rel="prefetch" href="/blog/assets/Maths.html-DfHnD4hV.js" as="script"><link rel="prefetch" href="/blog/assets/Statistics.html-7NcF3KG_.js" as="script"><link rel="prefetch" href="/blog/assets/qf formulas.html-3Euf0Wbr.js" as="script"><link rel="prefetch" href="/blog/assets/HPC.html-NkZV7U-2.js" as="script"><link rel="prefetch" href="/blog/assets/Learning_Makefile.html-B0ptyLbT.js" as="script"><link rel="prefetch" href="/blog/assets/cmd.html-C79zr-dg.js" as="script"><link rel="prefetch" href="/blog/assets/conda.html-ly7C6Y2-.js" as="script"><link rel="prefetch" href="/blog/assets/git.html-1BJ16TbZ.js" as="script"><link rel="prefetch" href="/blog/assets/linux.html-AnJ1LL7H.js" as="script"><link rel="prefetch" href="/blog/assets/styles.html-B7Tm7wJf.js" as="script"><link rel="prefetch" href="/blog/assets/Blog_conf.html-Bto3kyed.js" as="script"><link rel="prefetch" href="/blog/assets/Algorithms.html-BRqaf5hV.js" as="script"><link rel="prefetch" href="/blog/assets/1 Introduction.html-CMkE3h_Z.js" as="script"><link rel="prefetch" href="/blog/assets/10 Self-supervised learning.html-D1T7JUkk.js" as="script"><link rel="prefetch" href="/blog/assets/2 Image classification with linear classifiers.html-CMNTYaHF.js" as="script"><link rel="prefetch" href="/blog/assets/3 CNN.html-DvZr4has.js" as="script"><link rel="prefetch" href="/blog/assets/9 Transformer.html-BU9YQNRP.js" as="script"><link rel="prefetch" href="/blog/assets/Formula.html-BtMHLzYn.js" as="script"><link rel="prefetch" href="/blog/assets/SLDG - Introduction.html-BOPEXHN7.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-xzA9YzAH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-N1IuGP9v.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BcnpVFkj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CB4hCESQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BWRy1gVK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BL_cN8DX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CM4Ddkay.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BjWSsMDz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-D4Lwv0NY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B50bjFzm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BCmVd8-3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B-Nj5f6G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CNv-eM9Y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CcAYwbMy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CH7cHr3n.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DXkc7Tm8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ClN1KQcI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DKbEesSS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BzQBUYDD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BxvC-n4r.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ClWlAFEa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C5TknWWr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ERGVEr1P.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-GXRgw7eJ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/"><img class="vp-nav-logo" src="https://theme-hope-assets.vuejs.press/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Krigo&#39;s 博客</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/" aria-label="博客主页"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->博客主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="SLDG"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>SLDG<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/posts/MATH/SLDG/SLDG-Introduction.html" aria-label="/posts/MATH/SLDG/SLDG-Introduction.html"><!---->/posts/MATH/SLDG/SLDG-Introduction.html<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://theme-hope.vuejs.press/zh/" aria-label="V2 文档" rel="noopener noreferrer" target="_blank"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><!--]-->V2 文档<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/" aria-label="博客主页"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->博客主页<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/intro.html" aria-label="介绍页"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-circle-info" style=""></span><!--]-->介绍页<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">HPC</span><!----></p><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/posts/HPC/OpenMP/intro.html" aria-label="/posts/HPC/OpenMP/intro.html"><!---->/posts/HPC/OpenMP/intro.html<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/posts/HPC/OpenMP/OpenMP_upwind.html" aria-label="/posts/HPC/OpenMP/OpenMP_upwind.html"><!---->/posts/HPC/OpenMP/OpenMP_upwind.html<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">Computational Mathematics</span><!----></p><ul class="vp-sidebar-links"></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">SLDG</span><!----></p><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/posts/MATH/SLDG/1d_SLDG.html" aria-label="/posts/MATH/SLDG/1d_SLDG.html"><!---->/posts/MATH/SLDG/1d_SLDG.html<!----></a></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Understanding CNN Representations</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://mister-hope.com" target="_blank" rel="noopener noreferrer">Krigo</a></span><span property="author" content="Krigo"></span></span><!----><!----><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 5 分钟</span><meta property="timeRequired" content="PT5M"></span><!----><!----></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-activations">1. Activations</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-gradients">2. Gradients</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-fun-applications">3. Fun Applications</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-advanced-visualization-techniques">4. Advanced Visualization Techniques</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_5-implementation-insights">5. Implementation Insights</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#gram-matrix">Gram matrix</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#references">References</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><h1 id="understanding-cnn-representations" tabindex="-1"><a class="header-anchor" href="#understanding-cnn-representations"><span>Understanding CNN Representations</span></a></h1><p>Usually, the first layer across different architectures of CNN seeks to learn edges and colors. However, deep layers learns more complex features in high dimensions which are hard to visualize. We can visualize filters at higher layers but not interesting.</p><h2 id="_1-activations" tabindex="-1"><a class="header-anchor" href="#_1-activations"><span>1. <strong>Activations</strong></span></a></h2><p>The activation refers to the <strong>output of a specific neuron</strong> (or channel) after processing the input through the network. This output is the result of applying a series of operations, such as convolutions, non-linearities (e.g., ReLU), and pooling, on the input image or feature map.</p><ul><li><p><strong>Nearest Neighbors</strong>: fc</p><ul><li>Use L2 distance in the feature space to find similar images based on the final fully connected layer (e.g., FC7). <ul><li>Activations from a specific layer of the CNN (often the final fully connected layer, e.g., FC7 in AlexNet) are extracted for a dataset of images.</li><li>These activaStions are treated as feature vectors representing each image in a high-dimensional space.</li></ul></li><li>To find the nearest neighbors for a given image, the activation vector of that image is compared with others in the dataset using a similarity metric (e.g., Euclidean distance or cosine similarity).</li></ul></li><li><p><strong>Dimensionality Reduction</strong>: (PCA (Principal Component Analysis) or t-SNE.)</p><ul><li>High-dimensional activations from a CNN layer (e.g., 4096-dimensional vectors from FC7) are collected across many images.</li><li>Dimensionality reduction techniques((PCA) or t-SNE), reduce these activations to a lower-dimensional space (e.g., 2D or 3D).</li><li>The resulting low-dimensional representations are plotted, revealing clusters or patterns that correspond to semantic groupings in the original data.</li></ul></li><li><p><strong>Maximal Patches</strong>:</p><ul><li>Identify the most activating image patches for specific neurons or channels in intermediate feature maps.</li><li>Run many images through the network.</li><li>Visualize image patches that correspond to maximal activations (identifying the input patterns or image regions that lead to the <strong>highest output</strong> value for a specific neuron.).</li></ul></li><li><p><strong>Occlusion Sensitivity</strong>:</p><ul><li>Mask portions of the input image systematically and observe the effect on the network&#39;s predictions.</li><li>Identify critical image regions that influence the prediction. <img src="/blog/assets/Occlusion-DZfqSfQK.png" alt="img.png" loading="lazy"> e.g. The prob of pred elephant decreased (red) when masking the elephant, so the neural network is looking at the correct place.</li></ul></li></ul><h2 id="_2-gradients" tabindex="-1"><a class="header-anchor" href="#_2-gradients"><span>2. <strong>Gradients</strong></span></a></h2><ul><li><p><strong>Saliency (显著性) Maps</strong>:</p><ul><li>Use backpropagation to compute the gradient of the class score with respect to image pixels.</li><li>Highlight important regions in the image for the target class. <img src="/blog/assets/saliency1-CCvn_R3K.png" alt="img.png" loading="lazy"> 可以根据这个显著图，去无监督的分割出图像中的对象.然后我们就可以使用训练好的网络，来以某种方式实现对输入图像中与对象类别对应的部分的分割操作[2] <img src="/blog/assets/saliency2-DgA1sKxU.png" alt="img.png" loading="lazy"></li></ul></li><li><p><strong>Intermediate(中期) features via guided backprop</strong>: Find the part of an image that a neuron responds to.</p><ul><li>1.Select a feature map in an intermediate layer</li><li>2.Pass an input image through the CNN to compute activations for all layers, including the selected intermediate layer.</li><li>3.Compute the gradient of the selected neuron or channel&#39;s activation with respect to the input image. This gradient indicates how changes in the input image would affect the activation of the selected feature.</li><li>4.Modify the backpropagation algorithm by allowing only positive gradients to pass back through ReLU layers. <ul><li>During backpropagation, set the gradient to zero for any neurons that had a negative gradient or a negative activation in the forward pass.</li><li>This ensures that only features that positively contribute to the activation are highlighted.</li></ul></li><li>5.Use the computed gradients to create a &quot;saliency map&quot; that highlights the regions of the input image most relevant to the selected feature.</li></ul></li><li><p><strong>Class Visualization</strong>:</p><ul><li>Generate synthetic(合成的) images through gradient ascent(上升) <ul><li>Gradient Ascent: <ul><li>to maximize a function.</li><li>It iteratively adjusts the parameters in the direction that increases the value of the objective function (e.g., reward function).</li></ul></li></ul></li><li>Regularize these images to ensure they remain interpretable. <ul><li>simple regularizer, penalize L2 norm of generated image</li><li>better regularizer: penalize L2 norm of generated image, also during optimization periodically. -&gt; more realistic picture. <img src="/blog/assets/gradientAscent-CU7r4zgA.png" alt="img.png" loading="lazy"></li></ul></li></ul></li><li><p><strong>Adversarial(对抗性的) Examples</strong>:</p><ul><li>Slightly perturb(扰乱) the image to maximize the score for an incorrect class.</li><li>Expose vulnerabilities in the network&#39;s decision-making process.</li><li>alg: <ul><li>1.Start from an arbitrary image</li><li>2.Pick an arbitrary category</li><li>3.Modify the img via gradiend ascent to maximize the class score</li><li>4.Stop when the network is fooled.</li></ul></li></ul></li><li><p><strong>Feature Inversion</strong>:</p><ul><li>Reconstruct input images from feature vectors or activations at different layers.</li><li>Use regularization methods (e.g., Total Variation) to maintain natural appearance in the reconstructions.</li><li>It can be used to evaluate the importance of feature maps, and to prune the least important ones for network compression</li></ul></li></ul><h2 id="_3-fun-applications" tabindex="-1"><a class="header-anchor" href="#_3-fun-applications"><span>3. <strong>Fun Applications</strong></span></a></h2><ul><li><p><strong>DeepDream</strong>:</p><ul><li>Enhance the features of an existing image by iteratively amplifying activations at specific layers.</li><li>Employ tricks like jittering and multiscale processing to create fractal-like patterns.</li><li>alg: <ul><li><ol><li>Forward: compute activations at chosen layer</li></ol></li><li><ol start="2"><li>Set gradient of chosen layer equal to its activation</li></ol></li><li><ol start="3"><li>Backward: Compute gradient on image</li></ol></li><li><ol start="4"><li>Update image</li></ol></li></ul></li></ul></li><li><p><strong>Style Transfer</strong>:</p><ul><li>Combine features from a content image and style statistics (Gram matrices) from a style image.</li><li>Optimize a new image to match both content and style characteristics.</li><li>Match features from content image and Gram matrices from style image</li><li>Resizing style image before running style transfer algorithm can transfer different types of features</li><li>Mix style from multiple images by taking a weighted average of Gram matrices <img src="/blog/assets/styleTrans1-Y60pkK8W.png" alt="img.png" loading="lazy"></li><li>Fast Neural Style Transfer: solve the probl: many forward/backward passes through VGG: <img src="/blog/assets/styleTrans2-Cs7tOqJx.png" alt="img.png" loading="lazy"></li><li>Replacing batch normalization with instance normalization improves results. <img src="/blog/assets/norm-Dm_Qj1s9.png" alt="img.png" loading="lazy"></li></ul></li><li><p><strong>Texture Synthesis</strong>:</p><ul><li>Generate larger textures from small patches using Gram matrices to capture feature statistics.</li><li>Employ iterative optimization with pre-trained networks (e.g., VGG).</li></ul></li></ul><h2 id="_4-advanced-visualization-techniques" tabindex="-1"><a class="header-anchor" href="#_4-advanced-visualization-techniques"><span>4. <strong>Advanced Visualization Techniques</strong></span></a></h2><ul><li><p><strong>Guided Backpropagation</strong>:</p><ul><li>Compute gradients while suppressing negative influences through ReLU, leading to more refined visualizations.</li></ul></li><li><p><strong>Multi-Faceted Feature Visualization</strong>:</p><ul><li>Visualize multiple aspects of a single neuron using advanced optimization and regularization methods.</li><li>Discover diverse features learned by each neuron in deep layers.</li></ul></li></ul><h2 id="_5-implementation-insights" tabindex="-1"><a class="header-anchor" href="#_5-implementation-insights"><span>5. <strong>Implementation Insights</strong></span></a></h2><ul><li>Regularization Techniques: <ul><li>Use Gaussian blurring, gradient clipping, and center bias during optimization.</li></ul></li><li>Tools: <ul><li>Pre-trained CNNs (e.g., AlexNet, ResNet, VGG) and libraries like ConvNetJS are useful for these tasks.</li></ul></li><li>Applications: <ul><li>Analyze what different layers and neurons are learning, improve interpretability, and create visually appealing outputs.</li></ul></li></ul><h2 id="gram-matrix" tabindex="-1"><a class="header-anchor" href="#gram-matrix"><span>Gram matrix</span></a></h2><p>选择的特征表示通常是一个形状为(C, H, W)的三维张量，其中C是通道数量，H和W分别是特征图的高和宽。为了计算格拉姆矩阵，你首先需要将这个3D张量转换为一个2D矩阵，形状为(C, H*W)，你可以通过reshape或flatten操作来实现这一步。然后，你计算这个2D矩阵与其自身的转置的乘积，得到的就是格拉姆矩阵，其形状为(C, C)。每个元素都是相应两个通道的特征向量的内积，可以被理解为这两个特征在图像中的相关性或共现度。</p><p>如果你从特征图中抽取两个C维向量并计算它们的格拉姆矩阵，实际上你就在计算这两个向量的外积。这种方法常常被用来衡量两个向量之间的关系。</p><p>这样的话，格拉姆矩阵的元素是两个向量的内积，可以表示这两个向量之间的相关性。如果你将特征图中的每个位置看作一个独立的样本，那么这个格拉姆矩阵就可以看作这两个向量在所有样本上的协方差矩阵。这个协方差矩阵可以提供关于这两个向量分布和关系的信息。</p><p>例如，如果你正在处理图像，并且你的向量是来自卷积层的特征图，那么这个协方差矩阵就可以捕捉到图像中的纹理信息。在这种情况下，每个元素的值都表示了相应的两个特征通道在图像的空间布局中的相关性。</p><p>因此，计算特征图中两个C维向量的格拉姆矩阵可以帮助你理解这两个向量的关系，以及它们在数据中的分布和交互。这对于理解你的模型，以及设计新的模型和算法都是有用的。</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h2><ol><li>https://www.youtube.com/watch?v=G1hGwHVykDU&amp;t=1383s [2] https://github.com/Michael-Jetson/ML_DL_CV_with_pytorch/blob/main/ComputerVision/Lecture14_Visualizing_and_Understanding.md</li></ol></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope/edit/main/src/posts/AI/ComputerVision/8 CNN Representation.md" aria-label="在 GitHub 上编辑此页" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<!----></a></div><div class="vp-meta-item git-info"><!----><!----></div></footer><!----><!----><!----><!--]--></main><!--]--><!----></div><!--]--><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-B3e5LvWJ.js" defer></script>
  </body>
</html>
