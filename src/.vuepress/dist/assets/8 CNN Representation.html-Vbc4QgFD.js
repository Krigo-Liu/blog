import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,b as a,o as n}from"./app-B3e5LvWJ.js";const s="/blog/assets/Occlusion-DZfqSfQK.png",r="/blog/assets/saliency1-CCvn_R3K.png",l="/blog/assets/saliency2-DgA1sKxU.png",o="/blog/assets/gradientAscent-CU7r4zgA.png",c="/blog/assets/styleTrans1-Y60pkK8W.png",p="/blog/assets/styleTrans2-Cs7tOqJx.png",g="/blog/assets/norm-Dm_Qj1s9.png",u={};function h(m,e){return n(),t("div",null,e[0]||(e[0]=[a('<h1 id="understanding-cnn-representations" tabindex="-1"><a class="header-anchor" href="#understanding-cnn-representations"><span>Understanding CNN Representations</span></a></h1><p>Usually, the first layer across different architectures of CNN seeks to learn edges and colors. However, deep layers learns more complex features in high dimensions which are hard to visualize. We can visualize filters at higher layers but not interesting.</p><h2 id="_1-activations" tabindex="-1"><a class="header-anchor" href="#_1-activations"><span>1. <strong>Activations</strong></span></a></h2><p>The activation refers to the <strong>output of a specific neuron</strong> (or channel) after processing the input through the network. This output is the result of applying a series of operations, such as convolutions, non-linearities (e.g., ReLU), and pooling, on the input image or feature map.</p><ul><li><p><strong>Nearest Neighbors</strong>: fc</p><ul><li>Use L2 distance in the feature space to find similar images based on the final fully connected layer (e.g., FC7). <ul><li>Activations from a specific layer of the CNN (often the final fully connected layer, e.g., FC7 in AlexNet) are extracted for a dataset of images.</li><li>These activaStions are treated as feature vectors representing each image in a high-dimensional space.</li></ul></li><li>To find the nearest neighbors for a given image, the activation vector of that image is compared with others in the dataset using a similarity metric (e.g., Euclidean distance or cosine similarity).</li></ul></li><li><p><strong>Dimensionality Reduction</strong>: (PCA (Principal Component Analysis) or t-SNE.)</p><ul><li>High-dimensional activations from a CNN layer (e.g., 4096-dimensional vectors from FC7) are collected across many images.</li><li>Dimensionality reduction techniques((PCA) or t-SNE), reduce these activations to a lower-dimensional space (e.g., 2D or 3D).</li><li>The resulting low-dimensional representations are plotted, revealing clusters or patterns that correspond to semantic groupings in the original data.</li></ul></li><li><p><strong>Maximal Patches</strong>:</p><ul><li>Identify the most activating image patches for specific neurons or channels in intermediate feature maps.</li><li>Run many images through the network.</li><li>Visualize image patches that correspond to maximal activations (identifying the input patterns or image regions that lead to the <strong>highest output</strong> value for a specific neuron.).</li></ul></li><li><p><strong>Occlusion Sensitivity</strong>:</p><ul><li>Mask portions of the input image systematically and observe the effect on the network&#39;s predictions.</li><li>Identify critical image regions that influence the prediction. <img src="'+s+'" alt="img.png" loading="lazy"> e.g. The prob of pred elephant decreased (red) when masking the elephant, so the neural network is looking at the correct place.</li></ul></li></ul><h2 id="_2-gradients" tabindex="-1"><a class="header-anchor" href="#_2-gradients"><span>2. <strong>Gradients</strong></span></a></h2><ul><li><p><strong>Saliency (显著性) Maps</strong>:</p><ul><li>Use backpropagation to compute the gradient of the class score with respect to image pixels.</li><li>Highlight important regions in the image for the target class. <img src="'+r+'" alt="img.png" loading="lazy"> 可以根据这个显著图，去无监督的分割出图像中的对象.然后我们就可以使用训练好的网络，来以某种方式实现对输入图像中与对象类别对应的部分的分割操作[2] <img src="'+l+'" alt="img.png" loading="lazy"></li></ul></li><li><p><strong>Intermediate(中期) features via guided backprop</strong>: Find the part of an image that a neuron responds to.</p><ul><li>1.Select a feature map in an intermediate layer</li><li>2.Pass an input image through the CNN to compute activations for all layers, including the selected intermediate layer.</li><li>3.Compute the gradient of the selected neuron or channel&#39;s activation with respect to the input image. This gradient indicates how changes in the input image would affect the activation of the selected feature.</li><li>4.Modify the backpropagation algorithm by allowing only positive gradients to pass back through ReLU layers. <ul><li>During backpropagation, set the gradient to zero for any neurons that had a negative gradient or a negative activation in the forward pass.</li><li>This ensures that only features that positively contribute to the activation are highlighted.</li></ul></li><li>5.Use the computed gradients to create a &quot;saliency map&quot; that highlights the regions of the input image most relevant to the selected feature.</li></ul></li><li><p><strong>Class Visualization</strong>:</p><ul><li>Generate synthetic(合成的) images through gradient ascent(上升) <ul><li>Gradient Ascent: <ul><li>to maximize a function.</li><li>It iteratively adjusts the parameters in the direction that increases the value of the objective function (e.g., reward function).</li></ul></li></ul></li><li>Regularize these images to ensure they remain interpretable. <ul><li>simple regularizer, penalize L2 norm of generated image</li><li>better regularizer: penalize L2 norm of generated image, also during optimization periodically. -&gt; more realistic picture. <img src="'+o+'" alt="img.png" loading="lazy"></li></ul></li></ul></li><li><p><strong>Adversarial(对抗性的) Examples</strong>:</p><ul><li>Slightly perturb(扰乱) the image to maximize the score for an incorrect class.</li><li>Expose vulnerabilities in the network&#39;s decision-making process.</li><li>alg: <ul><li>1.Start from an arbitrary image</li><li>2.Pick an arbitrary category</li><li>3.Modify the img via gradiend ascent to maximize the class score</li><li>4.Stop when the network is fooled.</li></ul></li></ul></li><li><p><strong>Feature Inversion</strong>:</p><ul><li>Reconstruct input images from feature vectors or activations at different layers.</li><li>Use regularization methods (e.g., Total Variation) to maintain natural appearance in the reconstructions.</li><li>It can be used to evaluate the importance of feature maps, and to prune the least important ones for network compression</li></ul></li></ul><h2 id="_3-fun-applications" tabindex="-1"><a class="header-anchor" href="#_3-fun-applications"><span>3. <strong>Fun Applications</strong></span></a></h2><ul><li><p><strong>DeepDream</strong>:</p><ul><li>Enhance the features of an existing image by iteratively amplifying activations at specific layers.</li><li>Employ tricks like jittering and multiscale processing to create fractal-like patterns.</li><li>alg: <ul><li><ol><li>Forward: compute activations at chosen layer</li></ol></li><li><ol start="2"><li>Set gradient of chosen layer equal to its activation</li></ol></li><li><ol start="3"><li>Backward: Compute gradient on image</li></ol></li><li><ol start="4"><li>Update image</li></ol></li></ul></li></ul></li><li><p><strong>Style Transfer</strong>:</p><ul><li>Combine features from a content image and style statistics (Gram matrices) from a style image.</li><li>Optimize a new image to match both content and style characteristics.</li><li>Match features from content image and Gram matrices from style image</li><li>Resizing style image before running style transfer algorithm can transfer different types of features</li><li>Mix style from multiple images by taking a weighted average of Gram matrices <img src="'+c+'" alt="img.png" loading="lazy"></li><li>Fast Neural Style Transfer: solve the probl: many forward/backward passes through VGG: <img src="'+p+'" alt="img.png" loading="lazy"></li><li>Replacing batch normalization with instance normalization improves results. <img src="'+g+'" alt="img.png" loading="lazy"></li></ul></li><li><p><strong>Texture Synthesis</strong>:</p><ul><li>Generate larger textures from small patches using Gram matrices to capture feature statistics.</li><li>Employ iterative optimization with pre-trained networks (e.g., VGG).</li></ul></li></ul><h2 id="_4-advanced-visualization-techniques" tabindex="-1"><a class="header-anchor" href="#_4-advanced-visualization-techniques"><span>4. <strong>Advanced Visualization Techniques</strong></span></a></h2><ul><li><p><strong>Guided Backpropagation</strong>:</p><ul><li>Compute gradients while suppressing negative influences through ReLU, leading to more refined visualizations.</li></ul></li><li><p><strong>Multi-Faceted Feature Visualization</strong>:</p><ul><li>Visualize multiple aspects of a single neuron using advanced optimization and regularization methods.</li><li>Discover diverse features learned by each neuron in deep layers.</li></ul></li></ul><h2 id="_5-implementation-insights" tabindex="-1"><a class="header-anchor" href="#_5-implementation-insights"><span>5. <strong>Implementation Insights</strong></span></a></h2><ul><li>Regularization Techniques: <ul><li>Use Gaussian blurring, gradient clipping, and center bias during optimization.</li></ul></li><li>Tools: <ul><li>Pre-trained CNNs (e.g., AlexNet, ResNet, VGG) and libraries like ConvNetJS are useful for these tasks.</li></ul></li><li>Applications: <ul><li>Analyze what different layers and neurons are learning, improve interpretability, and create visually appealing outputs.</li></ul></li></ul><h2 id="gram-matrix" tabindex="-1"><a class="header-anchor" href="#gram-matrix"><span>Gram matrix</span></a></h2><p>选择的特征表示通常是一个形状为(C, H, W)的三维张量，其中C是通道数量，H和W分别是特征图的高和宽。为了计算格拉姆矩阵，你首先需要将这个3D张量转换为一个2D矩阵，形状为(C, H*W)，你可以通过reshape或flatten操作来实现这一步。然后，你计算这个2D矩阵与其自身的转置的乘积，得到的就是格拉姆矩阵，其形状为(C, C)。每个元素都是相应两个通道的特征向量的内积，可以被理解为这两个特征在图像中的相关性或共现度。</p><p>如果你从特征图中抽取两个C维向量并计算它们的格拉姆矩阵，实际上你就在计算这两个向量的外积。这种方法常常被用来衡量两个向量之间的关系。</p><p>这样的话，格拉姆矩阵的元素是两个向量的内积，可以表示这两个向量之间的相关性。如果你将特征图中的每个位置看作一个独立的样本，那么这个格拉姆矩阵就可以看作这两个向量在所有样本上的协方差矩阵。这个协方差矩阵可以提供关于这两个向量分布和关系的信息。</p><p>例如，如果你正在处理图像，并且你的向量是来自卷积层的特征图，那么这个协方差矩阵就可以捕捉到图像中的纹理信息。在这种情况下，每个元素的值都表示了相应的两个特征通道在图像的空间布局中的相关性。</p><p>因此，计算特征图中两个C维向量的格拉姆矩阵可以帮助你理解这两个向量的关系，以及它们在数据中的分布和交互。这对于理解你的模型，以及设计新的模型和算法都是有用的。</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h2><ol><li>https://www.youtube.com/watch?v=G1hGwHVykDU&amp;t=1383s [2] https://github.com/Michael-Jetson/ML_DL_CV_with_pytorch/blob/main/ComputerVision/Lecture14_Visualizing_and_Understanding.md</li></ol>',21)]))}const y=i(u,[["render",h],["__file","8 CNN Representation.html.vue"]]),v=JSON.parse(`{"path":"/posts/AI/ComputerVision/8%20CNN%20Representation.html","title":"Understanding CNN Representations","lang":"zh-CN","frontmatter":{"description":"Understanding CNN Representations Usually, the first layer across different architectures of CNN seeks to learn edges and colors. However, deep layers learns more complex featur...","head":[["meta",{"property":"og:url","content":"https://mister-hope.github.io/blog/posts/AI/ComputerVision/8%20CNN%20Representation.html"}],["meta",{"property":"og:site_name","content":"Krigo's 博客"}],["meta",{"property":"og:title","content":"Understanding CNN Representations"}],["meta",{"property":"og:description","content":"Understanding CNN Representations Usually, the first layer across different architectures of CNN seeks to learn edges and colors. However, deep layers learns more complex featur..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Krigo"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Understanding CNN Representations\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Krigo\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"1. Activations","slug":"_1-activations","link":"#_1-activations","children":[]},{"level":2,"title":"2. Gradients","slug":"_2-gradients","link":"#_2-gradients","children":[]},{"level":2,"title":"3. Fun Applications","slug":"_3-fun-applications","link":"#_3-fun-applications","children":[]},{"level":2,"title":"4. Advanced Visualization Techniques","slug":"_4-advanced-visualization-techniques","link":"#_4-advanced-visualization-techniques","children":[]},{"level":2,"title":"5. Implementation Insights","slug":"_5-implementation-insights","link":"#_5-implementation-insights","children":[]},{"level":2,"title":"Gram matrix","slug":"gram-matrix","link":"#gram-matrix","children":[]},{"level":2,"title":"References","slug":"references","link":"#references","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":5.27,"words":1581},"filePathRelative":"posts/AI/ComputerVision/8 CNN Representation.md","excerpt":"\\n<p>Usually, the first layer across different architectures of CNN seeks to learn edges and colors.\\nHowever, deep layers learns more complex features in high dimensions which are hard to visualize.\\nWe can visualize filters at higher layers but not interesting.</p>\\n<h2>1. <strong>Activations</strong></h2>","autoDesc":true}`);export{y as comp,v as data};
