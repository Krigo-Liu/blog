import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,b as n,o as r}from"./app-B3e5LvWJ.js";const a="/blog/assets/seq2seqAtt-DqSta2k3.png",i="/blog/assets/imgcapAtt-BN3qTwF-.png",o="/blog/assets/genatt-DXA2O64H.png",l="/blog/assets/selfatt-RbQuEJU1.png",d="/blog/assets/CNNselfatt-BhXzKEF0.png",c="/blog/assets/maskedselfatt-wTvdfWhd.png",g="/blog/assets/multiheadselfatt-BSPFIUn5.png",p="/blog/assets/pos-CB555ilP.png",h="/blog/assets/encoder_decoder-DZaot73t.png",m="/blog/assets/RNNvsTrans-DhR45wG-.png",u="/blog/assets/vit-7jMlBRSJ.png",f="/blog/assets/vitvscnn-C2DJpkhw.png",_="/blog/assets/vit-swin-BbxpicLE.png",b="/blog/assets/vit-swin2-DOF8HDrI.png",v="/blog/assets/vit-swin3-BDNzJnSl.png",y="/blog/assets/DETR-DL-spLmA.png",k={};function T(w,e){return r(),s("div",null,e[0]||(e[0]=[n('<h3 id="_1-recurrent-neural-networks-rnns-and-their-trade-offs" tabindex="-1"><a class="header-anchor" href="#_1-recurrent-neural-networks-rnns-and-their-trade-offs"><span>1. Recurrent Neural Networks (RNNs) and Their Trade-offs</span></a></h3><ul><li><strong>Advantages</strong>: <ul><li>Can process sequences of arbitrary length.</li><li>Shares weights across time, making it parameter-efficient.</li></ul></li><li><strong>Disadvantages</strong>: <ul><li>Sequential processing leads to slow computation.</li><li>Struggles with long-term dependencies due to vanishing gradients.</li></ul></li></ul><h3 id="_2-seq2seq-task" tabindex="-1"><a class="header-anchor" href="#_2-seq2seq-task"><span>2. seq2seq Task</span></a></h3><h4 id="_2-1-only-rnns" tabindex="-1"><a class="header-anchor" href="#_2-1-only-rnns"><span>2.1 only RNNs</span></a></h4><p>Problem: The encoder compresses the entire input sequence into a single fixed-size vector (the final hidden state of the encoder). This vector serves as the sole context for the decoder to generate the output sequence.For long input sequences, this fixed-size vector often fails to capture all relevant information, leading to loss of critical details. Decoders struggle to generate accurate outputs for longer or more complex sequences.</p><h4 id="_2-2-rnn-attention" tabindex="-1"><a class="header-anchor" href="#_2-2-rnn-attention"><span>2.2 RNN + <strong>Attention</strong></span></a></h4><ul><li>Attention addresses the bottleneck of fixed-size context vectors by allowing the decoder to focus on specific parts of the input sequence dynamically.</li><li><strong>Key Features</strong>: <ul><li><strong>Alignment Scores</strong></li><li><strong>Context Vector</strong><img src="'+a+'" alt="img.png" loading="lazy"></li><li>different context vector in each timestep of decoder</li><li>Input sequence not bottlenecked through single vector</li><li>At each timestep of decoder, context vector “looks at” different parts of the input sequence</li></ul></li></ul><h3 id="_3-image-captioning-task" tabindex="-1"><a class="header-anchor" href="#_3-image-captioning-task"><span>3. Image Captioning Task</span></a></h3><h4 id="_3-1-rnn-attention" tabindex="-1"><a class="header-anchor" href="#_3-1-rnn-attention"><span>3.1 RNN + <strong>Attention</strong></span></a></h4><p>New context vector at every time step. Each context vector will attend to different image regions. <img src="'+i+'" alt="img.png" loading="lazy"> This entire process is differentiable. Model chooses its own attention weights, no attention supervision is required.</p><h3 id="_4-general-att-v-s-self-att" tabindex="-1"><a class="header-anchor" href="#_4-general-att-v-s-self-att"><span>4. General att. V.S. Self att.</span></a></h3><table><thead><tr><th><strong>Aspect</strong></th><th><strong>General Attention Layer</strong></th><th><strong>Self-Attention Layer</strong></th></tr></thead><tbody><tr><td><strong>Purpose,focuses on relationship</strong></td><td>between two sets of vectors (e.g., encoder-decoder).</td><td>within the same sequence.</td></tr><tr><td><strong>Inputs</strong></td><td>Query (decoder), Keys and Values (encoder).</td><td>Query, Key, and Value are all derived from the same input.</td></tr><tr><td><strong>Context Vector</strong></td><td>information from the external input sequence.</td><td>relationships among tokens in the same sequence.</td></tr><tr><td><strong>Usage</strong></td><td>Used in encoder-decoder attention for seq2seq tasks.</td><td>Core of self-contained models like Transformers.</td></tr><tr><td><strong>Applications</strong></td><td>Machine translation, image captioning, multimodal tasks.</td><td>Language modeling, text generation, image processing.</td></tr><tr><td><strong>Complexity</strong></td><td>Depends on the sizes of the query and key-value sets.</td><td>Scales quadratically with sequence length.</td></tr></tbody></table><p><img src="'+o+'" alt="img.png" loading="lazy"><img src="'+l+'" alt="img.png" loading="lazy"> Self-attention layer doesn’t care about the orders of the inputs!</p><ul><li>Problem: How to encode ordered sequences like language adn spatially ordered img features?</li><li>Solution: Positional encoding <img src="'+d+'" alt="img.png" loading="lazy"></li></ul><h3 id="_5-transformers" tabindex="-1"><a class="header-anchor" href="#_5-transformers"><span>5. Transformers</span></a></h3><ol><li>Introduced in <strong>&quot;Attention Is All You Need&quot; (Vaswani et al., 2017)</strong>.</li><li>Transformers are a type of layer that uses self-attention and layer norm. ○ It is highly scalable and highly parallelizable ○ Faster training, larger models, better performance across vision and language tasks</li><li>Key Innovations: multi-head att self-att, positional encoding, masked att. <img src="'+c+'" alt="img.png" loading="lazy"><img src="'+g+'" alt="img.png" loading="lazy"><img src="'+p+'" alt="img.png" loading="lazy"><img src="'+h+'" alt="img.png" loading="lazy"><img src="'+m+'" alt="img.png" loading="lazy"></li></ol><h3 id="_6-vision-transformers-vits" tabindex="-1"><a class="header-anchor" href="#_6-vision-transformers-vits"><span>6. Vision Transformers (ViTs)</span></a></h3><ul><li>Adapt Transformers for image processing.</li><li>Split images into patches (e.g., (16 \\times 16)) and treat each patch as a sequence token.</li><li>Pretrained on large-scale image datasets, demonstrating competitive performance with CNNs. <img src="'+u+'" alt="img.png" loading="lazy"><img src="'+f+'" alt="img.png" loading="lazy"></li></ul><h4 id="_6-1-hierarchical-vit" tabindex="-1"><a class="header-anchor" href="#_6-1-hierarchical-vit"><span>6.1 Hierarchical ViT:</span></a></h4><p>MViT, Improved MViT, Swin (Swin Transformer: shifted window attention) Swin: <img src="'+_+'" alt="img.png" loading="lazy"><img src="'+b+'" alt="img.png" loading="lazy"><img src="'+v+'" alt="img.png" loading="lazy"></p><h3 id="_7-obj-detection-with-transformers-detr" tabindex="-1"><a class="header-anchor" href="#_7-obj-detection-with-transformers-detr"><span>7. Obj detection with Transformers: DETR</span></a></h3><p>Simple object detection pipeline: directly output a set of boxes from a Transformer Noanchors, no regression of box transforms Match predicted boxes to GT boxes with bipartite matching; train to regress box coordinates <img src="'+y+'" alt="img.png" loading="lazy"> Architecture of DETR:</p><ol><li>Backbone (Feature Extraction): A CNN (e.g., ResNet) is used as a backbone to extract feature maps from the input image.</li><li>Transformer Encoder: Processes the flattened feature maps and encodes the global context of the image.</li><li>Transformer Decoder: Takes learned positional embeddings (object queries) and interacts with the encoder outputs to predict objects.</li><li>Prediction Heads:</li></ol><ul><li>Each decoder output predicts: <ul><li>Class label: The category of the object.</li><li>Bounding box: The coordinates of the object&#39;s location, represented as normalized values.</li></ul></li></ul><p>+:</p><ol><li>End-to-End Training: Removes the need for complex heuristics like anchors, proposals, or NMS.</li><li>Global context:transformer captures global relationships in the image</li><li>flexibility: easily extended to other tasks like panoptic (全景式) segmentation.</li></ol><p>-:</p><ol><li>Slow Convergence</li><li>High Computational Cost</li><li>Localization Precision: Bounding box regression can sometimes be less precise compared to traditional methods.</li></ol><h3 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h3><p>https://cs231n.stanford.edu/slides/2022/lecture_10_ruohan.pdf https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf</p>',30)]))}const q=t(k,[["render",T],["__file","9 Transformer.html.vue"]]),S=JSON.parse(`{"path":"/posts/AI/ComputerVision/9%20Transformer.html","title":"","lang":"zh-CN","frontmatter":{"description":"1. Recurrent Neural Networks (RNNs) and Their Trade-offs Advantages: Can process sequences of arbitrary length. Shares weights across time, making it parameter-efficient. Disadv...","head":[["meta",{"property":"og:url","content":"https://mister-hope.github.io/blog/posts/AI/ComputerVision/9%20Transformer.html"}],["meta",{"property":"og:site_name","content":"Krigo's 博客"}],["meta",{"property":"og:description","content":"1. Recurrent Neural Networks (RNNs) and Their Trade-offs Advantages: Can process sequences of arbitrary length. Shares weights across time, making it parameter-efficient. Disadv..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Krigo"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Krigo\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":3,"title":"1. Recurrent Neural Networks (RNNs) and Their Trade-offs","slug":"_1-recurrent-neural-networks-rnns-and-their-trade-offs","link":"#_1-recurrent-neural-networks-rnns-and-their-trade-offs","children":[]},{"level":3,"title":"2. seq2seq Task","slug":"_2-seq2seq-task","link":"#_2-seq2seq-task","children":[]},{"level":3,"title":"3. Image Captioning Task","slug":"_3-image-captioning-task","link":"#_3-image-captioning-task","children":[]},{"level":3,"title":"4. General att. V.S. Self att.","slug":"_4-general-att-v-s-self-att","link":"#_4-general-att-v-s-self-att","children":[]},{"level":3,"title":"5. Transformers","slug":"_5-transformers","link":"#_5-transformers","children":[]},{"level":3,"title":"6. Vision Transformers (ViTs)","slug":"_6-vision-transformers-vits","link":"#_6-vision-transformers-vits","children":[]},{"level":3,"title":"7. Obj detection with Transformers: DETR","slug":"_7-obj-detection-with-transformers-detr","link":"#_7-obj-detection-with-transformers-detr","children":[]},{"level":3,"title":"References","slug":"references","link":"#references","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.29,"words":688},"filePathRelative":"posts/AI/ComputerVision/9 Transformer.md","excerpt":"<h3>1. Recurrent Neural Networks (RNNs) and Their Trade-offs</h3>\\n<ul>\\n<li><strong>Advantages</strong>:\\n<ul>\\n<li>Can process sequences of arbitrary length.</li>\\n<li>Shares weights across time, making it parameter-efficient.</li>\\n</ul>\\n</li>\\n<li><strong>Disadvantages</strong>:\\n<ul>\\n<li>Sequential processing leads to slow computation.</li>\\n<li>Struggles with long-term dependencies due to vanishing gradients.</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}`);export{q as comp,S as data};
