import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,o as i,b as l}from"./app-ZOsOiso-.js";const a="/blog/assets/parllel_computer-CWrlzqUO.png",o="/blog/assets/smp-BE90fEvA.png",r="/blog/assets/hpc-B-5IQl7K.png",s="/blog/assets/simd-B3udqY8L.png",n="/blog/assets/concurrent_nonparallel-BnvwBmSH.png",p="/blog/assets/cocurrent_parallel-Bp4uWR4y.png",c="/blog/assets/distributed_system-DSp_RJKW.png",u="/blog/assets/data_storage-BMbRRGLz.png",m={},d=l('<h1 id="what-is-a-parallel-computer" tabindex="-1"><a class="header-anchor" href="#what-is-a-parallel-computer"><span>What is a Parallel Computer?</span></a></h1><figure><img src="'+a+'" alt="Parallel Computer" tabindex="0" loading="lazy"><figcaption>Parallel Computer</figcaption></figure><h2 id="smp-shared-memory-mut-multiprocessor" tabindex="-1"><a class="header-anchor" href="#smp-shared-memory-mut-multiprocessor"><span>SMP - Shared Memory/Mut Multiprocessor</span></a></h2><figure><img src="'+o+'" alt="SMP" tabindex="0" loading="lazy"><figcaption>SMP</figcaption></figure><p>A <strong>SMP</strong> by connecting multiple processors to a single memory system</p><ul><li>A <strong>multicore processor</strong> contains multiple processors (cores) on a single chip. —— SMP的子类</li><li>广义上来讲，其实不需要集成在一个chip上</li></ul><p>因此，本质上，我们日常所使用的PC (Personal Computer) 就是所谓的SMP</p><h2 id="hpc-high-performace-computing-distributed-memory" tabindex="-1"><a class="header-anchor" href="#hpc-high-performace-computing-distributed-memory"><span>HPC - High Performace Computing / Distributed Memory</span></a></h2><figure><img src="'+r+'" alt="HPC" tabindex="0" loading="lazy"><figcaption>HPC</figcaption></figure><ul><li>A <strong>distributed memory multiprocessor</strong> has processors with their own memories connected by a high speed network. <ul><li>Also called a cluster</li><li>这个本质上就是多个CPU并联， <ul><li>但是物理上具体是连接到同一个主板，还是简单的多个电脑物理连接还未知</li></ul></li></ul></li><li>A <strong>high performace com puting</strong> system contains 100s or 1000s of such processors (nodes)</li></ul><h2 id="simd-single-processor-multiple-data" tabindex="-1"><a class="header-anchor" href="#simd-single-processor-multiple-data"><span>SIMD - Single Processor Multiple Data</span></a></h2><figure><img src="'+s+'" alt="SIMD" tabindex="0" loading="lazy"><figcaption>SIMD</figcaption></figure><ul><li>A <strong>Single Processor Multiple Data</strong> computer has multiple processors (or functional units) that perform the same operation on multiple data elements at once.</li><li>Most single processors have SIMD units with 2-8 way parallelism</li><li>Graphics processing units (GPUs) use this.</li></ul><h1 id="questions" tabindex="-1"><a class="header-anchor" href="#questions"><span>Questions</span></a></h1><ul><li>Why is high-performace computing often synonymous (同义的) with parallel computing? <ul><li><strong>Answer: Performance = parallelism</strong></li></ul></li><li>Why do we care so much about interconnect (互连) and communication? <ul><li>SMP, HPC, and SIMD are how thses processors are connected in different ways</li><li><strong>Answer: Efficiency = locality</strong></li></ul></li></ul><blockquote><p><strong>Performance = parallelism Efficiency = locality —— Bill Dally (NVIDIA and Standford)</strong></p></blockquote><h1 id="what-s-not-a-parallel-computer" tabindex="-1"><a class="header-anchor" href="#what-s-not-a-parallel-computer"><span>What’s not a Parallel Computer?</span></a></h1><h2 id="concurrency-并发性-vs-parallelism" tabindex="-1"><a class="header-anchor" href="#concurrency-并发性-vs-parallelism"><span>Concurrency (并发性) vs. Parallelism</span></a></h2><ul><li>Concurrency: multiple tasks are <em><strong>logically</strong></em> active at one time. <ul><li>Example: 你在电脑上打开一个网页，同时在放音乐，并且微信在接受消息</li><li>这里会让你产生在并行的错觉</li><li>这种本质上称作任务的并行 —— 并发</li></ul></li></ul><figure><img src="'+n+'" alt="Concurrent non-parallel" tabindex="0" loading="lazy"><figcaption>Concurrent non-parallel</figcaption></figure><ul><li>Parallelism: multiple tasks are <em><strong>actually</strong></em> active at one time. <ul><li>Example: 你在计算 1+1 = 2 的同时，计算 2+2 = 4 <ul><li>因此，如果你只有一个线程，这两件事会有冲突。但并发则会因为延迟过低的原因让你产生两件事情是同时进行的错觉。</li></ul></li></ul></li></ul><figure><img src="'+p+'" alt="Concurrent parallel" tabindex="0" loading="lazy"><figcaption>Concurrent parallel</figcaption></figure><h2 id="parallel-computer-vs-distributed-system" tabindex="-1"><a class="header-anchor" href="#parallel-computer-vs-distributed-system"><span>Parallel Computer vs. Distributed System</span></a></h2><ul><li>A distributed system is <em><strong>inherently (本质上)</strong></em> distributed, i.e., serving clients (客户) at different locations.</li></ul><figure><img src="'+c+'" alt="single monolithic entity" tabindex="0" loading="lazy"><figcaption>single monolithic entity</figcaption></figure><p>single monolithic entity</p><ul><li>A parallel computer may use <em><strong>distributed memory</strong></em> (multiple processors with their own memory) for more performance.</li><li>In reality, even servers are distributed to make sure that when one component fails, you stills can access online banking account.</li><li>在现实中，分布式系统广泛的应用于服务器，我们所熟知的淘宝，拼多多等，都是这类型。这种类型可以保证短时间内大量客户进行访问。</li></ul><h1 id="units-of-measure-for-hpc" tabindex="-1"><a class="header-anchor" href="#units-of-measure-for-hpc"><span>Units of Measure for HPC</span></a></h1><ul><li><p>High Performance Computing units are:</p><ul><li>Flop: floating point operation, usually double precision unless noted</li><li>Flop/s: floating point operations per second</li><li>Bytes: size of data (a double percision floating point number is 8 bytes)</li></ul></li><li><p>Typical sizes are millions…</p><figure><img src="'+u+'" alt="Units of Measure for HPC" tabindex="0" loading="lazy"><figcaption>Units of Measure for HPC</figcaption></figure></li><li><p>Current fastest machines:</p><p><a href="http://www.top500.org" target="_blank" rel="noopener noreferrer">Home - | TOP500</a></p></li></ul><h1 id="simulation-hpc" tabindex="-1"><a class="header-anchor" href="#simulation-hpc"><span>Simulation &amp;&amp; HPC</span></a></h1><p>High Performance simulation used to understanding things that are:</p><ul><li>Too big</li><li>Too small</li><li>Too fast</li><li>Too slow</li><li>Too expensive or dangerous</li></ul><p>for experiments</p><h1 id="data-analytics-hpc" tabindex="-1"><a class="header-anchor" href="#data-analytics-hpc"><span>Data Analytics &amp;&amp; HPC</span></a></h1><p>High Performance Data Analytics (HPDA) is used for data sets that are:</p><ul><li>Too big</li><li>Too complex</li><li>Too fast</li><li>Too noisy</li><li>Too heterogeneous</li></ul><p>for measurement alone.</p><h1 id="amdahl-s-law" tabindex="-1"><a class="header-anchor" href="#amdahl-s-law"><span>Amdahl’s Law</span></a></h1><ul><li><p>Suppose only part of an application is parallel</p></li><li><p>Amdahl’s law</p><ul><li>s = fraction of work done sequentially (Amdahl fraction)</li><li>1-s is fraction parallelizable</li><li>P = number of processors</li></ul><aside> ⚠️ Speedup(P) = Time(1)/Time(P) ≤ 1/(s+(1-s)/P) ≤ 1/s </aside><ul><li>Even if the parallel part speeds up perfectly, performance is limited by the sequential part <ul><li>无论你做的多好，加速都不是线性</li></ul></li></ul></li></ul>',39),g=[d];function h(f,y){return i(),t("div",null,g)}const C=e(m,[["render",h],["__file","CS267 - Lecture 1 Inotroduction 925af07b110845fa8ab485d1df6d877e.html.vue"]]),S=JSON.parse(`{"path":"/posts/HPC/CS267/CS267%20-%20Lecture%201%20Inotroduction%20925af07b110845fa8ab485d1df6d877e.html","title":"UCB 267 Lecture 1 Inotroduction","lang":"zh-CN","frontmatter":{"title":"UCB 267 Lecture 1 Inotroduction","icon":"file","order":3,"author":"Krigo","category":["CS"],"tag":["CS","Parallel Computing","Coding"],"footer":"Thank's myself until I find a jinx to ask questions. haha","copyrigh":"无版权","date":"2024-06-23T00:00:00.000Z","description":"What is a Parallel Computer? Parallel ComputerParallel Computer SMP - Shared Memory/Mut Multiprocessor SMPSMP A SMP by connecting multiple processors to a single memory system A...","head":[["meta",{"property":"og:url","content":"https://mister-hope.github.io/blog/posts/HPC/CS267/CS267%20-%20Lecture%201%20Inotroduction%20925af07b110845fa8ab485d1df6d877e.html"}],["meta",{"property":"og:site_name","content":"Krigo's 博客"}],["meta",{"property":"og:title","content":"UCB 267 Lecture 1 Inotroduction"}],["meta",{"property":"og:description","content":"What is a Parallel Computer? Parallel ComputerParallel Computer SMP - Shared Memory/Mut Multiprocessor SMPSMP A SMP by connecting multiple processors to a single memory system A..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Krigo"}],["meta",{"property":"article:tag","content":"CS"}],["meta",{"property":"article:tag","content":"Parallel Computing"}],["meta",{"property":"article:tag","content":"Coding"}],["meta",{"property":"article:published_time","content":"2024-06-23T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"UCB 267 Lecture 1 Inotroduction\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-06-23T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Krigo\\"}]}"]]},"headers":[{"level":2,"title":"SMP - Shared Memory/Mut Multiprocessor","slug":"smp-shared-memory-mut-multiprocessor","link":"#smp-shared-memory-mut-multiprocessor","children":[]},{"level":2,"title":"HPC - High Performace Computing / Distributed Memory","slug":"hpc-high-performace-computing-distributed-memory","link":"#hpc-high-performace-computing-distributed-memory","children":[]},{"level":2,"title":"SIMD - Single Processor Multiple Data","slug":"simd-single-processor-multiple-data","link":"#simd-single-processor-multiple-data","children":[]},{"level":2,"title":"Concurrency (并发性) vs. Parallelism","slug":"concurrency-并发性-vs-parallelism","link":"#concurrency-并发性-vs-parallelism","children":[]},{"level":2,"title":"Parallel Computer vs. Distributed System","slug":"parallel-computer-vs-distributed-system","link":"#parallel-computer-vs-distributed-system","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.55,"words":766},"filePathRelative":"posts/HPC/CS267/CS267 - Lecture 1 Inotroduction 925af07b110845fa8ab485d1df6d877e.md","localizedDate":"2024年6月23日","excerpt":"\\n<figure><figcaption>Parallel Computer</figcaption></figure>\\n<h2>SMP - Shared Memory/Mut Multiprocessor</h2>\\n<figure><figcaption>SMP</figcaption></figure>\\n<p>A <strong>SMP</strong> by connecting multiple processors to a single memory system</p>\\n<ul>\\n<li>A <strong>multicore processor</strong> contains multiple processors (cores) on a single chip. —— SMP的子类</li>\\n<li>广义上来讲，其实不需要集成在一个chip上</li>\\n</ul>","autoDesc":true}`);export{C as comp,S as data};
