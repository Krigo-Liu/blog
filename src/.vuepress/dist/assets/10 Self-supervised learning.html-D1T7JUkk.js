import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,b as a,o as s}from"./app-B3e5LvWJ.js";const n="/blog/assets/lec10core-B6ZNDLrp.png",l="/blog/assets/MAE-bzR6JbVc.png",o="/blog/assets/color-DId-PwkW.png",r="/blog/assets/color-splitb-CUfN6IyF.png",c="/blog/assets/cons_data_aug-CXLERR7J.png",d="/blog/assets/CLIP-Ve5-zagi.png",p={};function g(u,e){return s(),t("div",null,e[0]||(e[0]=[a('<p><strong>Background</strong>:</p><ul><li><strong>Supervised Learning</strong> relies on labeled data: costly and time-consuming to annotate.</li><li><strong>Unsupervised Learning</strong> seeks patterns in unlabeled data but doesn&#39;t predict specific outcomes.</li><li><strong>Semi-Supervised Learning</strong>: Train jointly with some labeled data and (a lot) of unlabeled data.</li></ul><hr><h3 id="_1-core-concepts" tabindex="-1"><a class="header-anchor" href="#_1-core-concepts"><span>1. Core Concepts</span></a></h3><p><strong>Self-Supervised Learning (SSL)</strong>:</p><ul><li>Definition: Learning from raw data without annotations by using pretext tasks.</li><li>Advantages: Reduces dependency on labeled data, improves scalability.</li><li>Key Steps: Pretraining (on pretext tasks) ‚Üí Transfer to downstream tasks.</li><li>Types of Pretext Tasks: <img src="'+n+'" alt="img.png" loading="lazy"></li></ul><h4 id="_1-1-generative-tasks" tabindex="-1"><a class="header-anchor" href="#_1-1-generative-tasks"><span>1.1 Generative Tasks</span></a></h4><ol><li><p>Sparse AutoEncoders: Train an autoencoder to reconstruct inputs with sparse activations (mostly 0).</p></li><li><p>Denoising Autoencoder: Train an autoencoder to reconstruct noisy inputs (pixels randomly set to zero)</p></li><li><p>Masked Autoencoder: Masks portions of the input (e.g., image patches) and learns to reconstruct the missing parts.</p><ul><li>Efficient and scalable for large models like Vision Transformers (ViTs). <img src="'+l+'" alt="img.png" loading="lazy"></li></ul></li><li><p>Inpainting: Predict missing regions in an image. encoder + decoder</p><ol><li>Mask a portion of the input image (e.g., rectangular or irregular regions).</li><li>Train the model to predict the missing pixels based on the visible parts of the image.</li><li>Use a loss function (e.g., L2 loss (+ adversarial loss <em>best</em>)) to evaluate the quality of the reconstruction.</li></ol><ul><li>strength: rich feature learining, global+local context</li><li>lim: specific scenarios, computational cost, dependence on masking strat, generative overhead (precise pixel pred, may not align with downstream tasks)</li></ul></li><li><p>Colorization:</p><ol><li>For grayscale images, identify objects, then Predict colors. <img src="'+o+'" alt="img.png" loading="lazy"></li><li>Split-Brain Autoencoder <img src="'+r+'" alt="img.png" loading="lazy"></li></ol></li></ol><h4 id="_1-2-discriminative-tasks" tabindex="-1"><a class="header-anchor" href="#_1-2-discriminative-tasks"><span>1.2 Discriminative Tasks</span></a></h4><ol><li>Contrastive Learning: Distinguish between similar (positive) and dissimilar (negative) samples. <ul><li>Contrastive loss: Each image predicts which caption matches</li><li>similarity: Euclidean distance</li><li>generate positive pairs: <ul><li>data augmentation</li><li><img src="'+c+'" alt="img.png" tabindex="0" loading="lazy"><figcaption>img.png</figcaption></li></ul></li></ul></li><li>Context Prediction: Predict spatial &amp; semantic relationships between image patches. tasks, like predicting the <strong>relative</strong> location of one patch to another. Eg: Given two image patches, predict whether one patch is &quot;above,&quot; &quot;below,&quot; &quot;to the left,&quot; or &quot;to the right&quot; of the other. <strong>Subset: Jigsaw (ÊãºÂõæ)</strong>: shuffling multiple patches, and pred <strong>permutation</strong> of the patches. <ul><li>lim: only work on patches, not whole images! Because patches are localized regions, struggles to model holistic features like obj presence, size or overall composition.</li></ul></li><li>Rotation Prediction:RotNet, Classify the degree of rotation (e.g., 0¬∞, 90¬∞, 180¬∞, 270¬∞) applied to an image.</li><li>Deep Clustering: Group features and assign pseudo-labels to clusters. <ol><li>steps 1 Randomly initialize a CNN 2 Run many images through CNN, get their final-layer features 3 Cluster the features with K-Means; record cluster for each feature 4 Use cluster assignments as <em>pseudo labels</em> for each image; train the CNN to predict cluster assignments 5 repeat, GOTO 2</li></ol></li><li>Exemplar CNN: recognize the same data instance (e.g., image patches) despite different augmentations like cropping, rotation, scaling, or color jittering. <ul><li>obj:The model predicts which of the original image patches each augmented sample came from (an N-way classification task, where ùëÅ is the number of original patches).</li><li>strengths: instance-level features, augmentation-invariance, task-agnostic</li><li>lim: scalability <ul><li>(The final classification layer grows with the number of instances, making it harder to scale to massive datasets.)</li><li>Risk of Over-fitting</li></ul></li></ul></li></ol><h4 id="_1-3-multimodal-pretext-tasks" tabindex="-1"><a class="header-anchor" href="#_1-3-multimodal-pretext-tasks"><span>1.3 Multimodal Pretext Tasks</span></a></h4><ol><li>CLIP (Contrastive Language-Image Pretraining): Match images with their corresponding text descriptions. <ul><li>why lang.: <ul><li>semantic density, universality(des any concept), scalability(Non-experts can easily caption images; data can also be collected from the web at scale).</li><li>Language enables <strong>zero-shot classification</strong>: the ability of a model to classify data into categories it has never explicitly been trained on. Instead, the model relies on its pretraining and generalization abilities to infer relationships between new data and previously learned representations. <img src="'+d+'" alt="img.png" loading="lazy">Dual encoders for image and text.</li><li>Strengths: <ul><li>Generalization across tasks. <ul><li>muti-modal, link vision and lang.</li></ul></li></ul></li><li>drawback: <ul><li>Dataset bias and closed nature of training data.</li><li>Dependency on large-scale data and compute.</li></ul></li></ul></li></ul></li><li>Video Context: Predict relationships between frames and their audio tracks.</li><li>Sound Correspondence: Learn relationships between an image and its associated ambient sound.</li></ol><hr><h3 id="_2-theoretical-insights-and-challenges" tabindex="-1"><a class="header-anchor" href="#_2-theoretical-insights-and-challenges"><span>2. Theoretical Insights and Challenges</span></a></h3><ol><li>Fair evaluation of SSL methods is very hard! No theory, so we need to rely on experiment</li><li>Variability in experimental setups: architecture, datasets, hyperparameters.</li><li>Current reliance on curated (data that has been carefully selected, organized, and preprocessed to meet specific criteria or objectives. This contrasts with raw or unprocessed data, which may include irrelevant, noisy, or unbalanced information.) datasets (like ImageNet).</li><li>Challenges scaling beyond curated or isolated datasets.</li></ol><hr><h3 id="_3-potential-exam-questions" tabindex="-1"><a class="header-anchor" href="#_3-potential-exam-questions"><span>3. Potential Exam Questions</span></a></h3><ol><li>Explain the key differences between supervised, unsupervised, and self-supervised learning.</li><li>Describe how contrastive learning works and its importance in SSL.</li><li>Explain how CLIP performs zero-shot classification.</li><li>Compare masked autoencoders with traditional autoencoders in terms of architecture and use cases.</li><li>Discuss the advantages and challenges of using SSL in multimodal learning.</li><li>Evaluate the trade-offs between generative and discriminative pretext tasks in SSL.</li></ol><hr><h3 id="study-tips" tabindex="-1"><a class="header-anchor" href="#study-tips"><span>Study Tips</span></a></h3><ul><li>Focus on understanding the logic behind pretext tasks and why they enable feature learning.</li><li>Memorize key architectures and loss functions (e.g., contrastive loss, InfoNCE).</li><li>Practice explaining applications like CLIP and masked autoencoders in simple terms.</li><li>Be ready to critique SSL methods, especially around scalability and bias.</li></ul><h3 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h3><p>https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/</p>',23)]))}const f=i(p,[["render",g],["__file","10 Self-supervised learning.html.vue"]]),b=JSON.parse(`{"path":"/posts/AI/ComputerVision/10%20Self-supervised%20learning.html","title":"","lang":"zh-CN","frontmatter":{"description":"Background: Supervised Learning relies on labeled data: costly and time-consuming to annotate. Unsupervised Learning seeks patterns in unlabeled data but doesn't predict specifi...","head":[["meta",{"property":"og:url","content":"https://mister-hope.github.io/blog/posts/AI/ComputerVision/10%20Self-supervised%20learning.html"}],["meta",{"property":"og:site_name","content":"Krigo's ÂçöÂÆ¢"}],["meta",{"property":"og:description","content":"Background: Supervised Learning relies on labeled data: costly and time-consuming to annotate. Unsupervised Learning seeks patterns in unlabeled data but doesn't predict specifi..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Krigo"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Krigo\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":3,"title":"1. Core Concepts","slug":"_1-core-concepts","link":"#_1-core-concepts","children":[]},{"level":3,"title":"2. Theoretical Insights and Challenges","slug":"_2-theoretical-insights-and-challenges","link":"#_2-theoretical-insights-and-challenges","children":[]},{"level":3,"title":"3. Potential Exam Questions","slug":"_3-potential-exam-questions","link":"#_3-potential-exam-questions","children":[]},{"level":3,"title":"Study Tips","slug":"study-tips","link":"#study-tips","children":[]},{"level":3,"title":"References","slug":"references","link":"#references","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.92,"words":875},"filePathRelative":"posts/AI/ComputerVision/10 Self-supervised learning.md","excerpt":"<p><strong>Background</strong>:</p>\\n<ul>\\n<li><strong>Supervised Learning</strong> relies on labeled data: costly and time-consuming to annotate.</li>\\n<li><strong>Unsupervised Learning</strong> seeks patterns in unlabeled data but doesn't predict specific outcomes.</li>\\n<li><strong>Semi-Supervised Learning</strong>: Train jointly with some labeled data and (a lot) of unlabeled data.</li>\\n</ul>","autoDesc":true}`);export{f as comp,b as data};
